{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\n\ncolumns = ['id', 'sentence 1', 'sentence 2', 'gold label']\ntraining_data = '/kaggle/input/mlfinal/MLFinalProject-main/train_with_label.txt'\ndevelopment_data = '/kaggle/input/mlfinal/MLFinalProject-main/dev_with_label.txt'\ntest_data = '/kaggle/input/mlfinal/MLFinalProject-main/test_without_label.txt'\n\ndf_test = pd.read_csv(test_data, sep = '\\t+', names = ['id','sentence 1', 'sentence 2'])#.apply(lambda x: x.astype(str).str.lower())\n\ndf_dev = pd.read_csv(development_data, sep = '\\t+', names = columns)#.apply(lambda x: x.astype(str).str.lower())\n\ndf_dev['gold label'] = pd.to_numeric(df_dev['gold label'], errors='coerce')\ndf_dev = df_dev.dropna().reset_index(drop = True)\ndf_dev['gold label'] = df_dev['gold label'].astype(int)\n\ndf = pd.read_csv(training_data, sep = '\\t+', names = columns)#.apply(lambda x: x.astype(str).str.lower())\n\ndf['gold label'] = pd.to_numeric(df['gold label'], errors='coerce')\ndf = df.dropna()\ndf['gold label'] = df['gold label'].astype(int)\n\n#Lists information of columns and respective data types\ndf.info() \ndf_dev.info() ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-07T04:18:25.381519Z","iopub.execute_input":"2022-12-07T04:18:25.382503Z","iopub.status.idle":"2022-12-07T04:18:25.581729Z","shell.execute_reply.started":"2022-12-07T04:18:25.382431Z","shell.execute_reply":"2022-12-07T04:18:25.579856Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7801 entries, 0 to 7800\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          7801 non-null   object\n 1   sentence 1  7801 non-null   object\n 2   sentence 2  7801 non-null   object\n 3   gold label  7801 non-null   int64 \ndtypes: int64(1), object(3)\nmemory usage: 304.7+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4000 entries, 0 to 3999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          4000 non-null   object\n 1   sentence 1  4000 non-null   object\n 2   sentence 2  4000 non-null   object\n 3   gold label  4000 non-null   int64 \ndtypes: int64(1), object(3)\nmemory usage: 125.1+ KB\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  return func(*args, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:25.584892Z","iopub.execute_input":"2022-12-07T04:18:25.585794Z","iopub.status.idle":"2022-12-07T04:18:25.609226Z","shell.execute_reply.started":"2022-12-07T04:18:25.585736Z","shell.execute_reply":"2022-12-07T04:18:25.606566Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"           id                                         sentence 1  \\\n0  train_id_0  Is it in the food supply ? \" says David Ropeik...   \n1  train_id_1  Hundreds of soldiers were involved , an appare...   \n2  train_id_2  And Sen. Michael Crapo , R-Idaho , chairman of...   \n3  train_id_3  The gunman , 26-year-old Harold Kilpatrick jnr...   \n4  train_id_4  The League of United Latin American Citizens ,...   \n\n                                          sentence 2  gold label  \n0  The pound also made progress against the dolla...           0  \n1  Avants , wearing a light brown jumpsuit , had ...           0  \n2  . 's Kempthorne of friend longtime a is , nomi...           0  \n3  \" In fact , I was physically sick several time...           0  \n4  No. 2 HP saw its Unix server sales dropped 3.6...           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentence 1</th>\n      <th>sentence 2</th>\n      <th>gold label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_id_0</td>\n      <td>Is it in the food supply ? \" says David Ropeik...</td>\n      <td>The pound also made progress against the dolla...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_id_1</td>\n      <td>Hundreds of soldiers were involved , an appare...</td>\n      <td>Avants , wearing a light brown jumpsuit , had ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_id_2</td>\n      <td>And Sen. Michael Crapo , R-Idaho , chairman of...</td>\n      <td>. 's Kempthorne of friend longtime a is , nomi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_id_3</td>\n      <td>The gunman , 26-year-old Harold Kilpatrick jnr...</td>\n      <td>\" In fact , I was physically sick several time...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_id_4</td>\n      <td>The League of United Latin American Citizens ,...</td>\n      <td>No. 2 HP saw its Unix server sales dropped 3.6...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Count how many words are in each sentence\n\ndf['words_1'] = df['sentence 1'].str.split().str.len()\ndf['words_2'] = df['sentence 2'].str.split().str.len()\ndf['wordcount difference'] = abs(df['sentence 1'].str.split().str.len() - df['sentence 2'].str.split().str.len())\n\n\ndf_dev['words_1'] = df_dev['sentence 1'].str.split().str.len()\ndf_dev['words_2'] = df_dev['sentence 2'].str.split().str.len()\ndf_dev['wordcount difference'] = abs(df_dev['sentence 1'].str.split().str.len() - df_dev['sentence 2'].str.split().str.len())\n\ndf_test['words_1'] = df_test['sentence 1'].str.split().str.len()\ndf_test['words_2'] = df_test['sentence 2'].str.split().str.len()\ndf_test['wordcount difference'] = abs(df_test['sentence 1'].str.split().str.len() - df_test['sentence 2'].str.split().str.len())\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:25.611092Z","iopub.execute_input":"2022-12-07T04:18:25.611946Z","iopub.status.idle":"2022-12-07T04:18:26.123995Z","shell.execute_reply.started":"2022-12-07T04:18:25.611901Z","shell.execute_reply":"2022-12-07T04:18:26.122662Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"           id                                         sentence 1  \\\n0  train_id_0  Is it in the food supply ? \" says David Ropeik...   \n1  train_id_1  Hundreds of soldiers were involved , an appare...   \n2  train_id_2  And Sen. Michael Crapo , R-Idaho , chairman of...   \n3  train_id_3  The gunman , 26-year-old Harold Kilpatrick jnr...   \n4  train_id_4  The League of United Latin American Citizens ,...   \n\n                                          sentence 2  gold label  words_1  \\\n0  The pound also made progress against the dolla...           0       24   \n1  Avants , wearing a light brown jumpsuit , had ...           0       23   \n2  . 's Kempthorne of friend longtime a is , nomi...           0       27   \n3  \" In fact , I was physically sick several time...           0       25   \n4  No. 2 HP saw its Unix server sales dropped 3.6...           0       29   \n\n   words_2  wordcount difference  \n0       17                     7  \n1       44                    21  \n2       27                     0  \n3       49                    24  \n4       16                    13  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentence 1</th>\n      <th>sentence 2</th>\n      <th>gold label</th>\n      <th>words_1</th>\n      <th>words_2</th>\n      <th>wordcount difference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_id_0</td>\n      <td>Is it in the food supply ? \" says David Ropeik...</td>\n      <td>The pound also made progress against the dolla...</td>\n      <td>0</td>\n      <td>24</td>\n      <td>17</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_id_1</td>\n      <td>Hundreds of soldiers were involved , an appare...</td>\n      <td>Avants , wearing a light brown jumpsuit , had ...</td>\n      <td>0</td>\n      <td>23</td>\n      <td>44</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_id_2</td>\n      <td>And Sen. Michael Crapo , R-Idaho , chairman of...</td>\n      <td>. 's Kempthorne of friend longtime a is , nomi...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_id_3</td>\n      <td>The gunman , 26-year-old Harold Kilpatrick jnr...</td>\n      <td>\" In fact , I was physically sick several time...</td>\n      <td>0</td>\n      <td>25</td>\n      <td>49</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_id_4</td>\n      <td>The League of United Latin American Citizens ,...</td>\n      <td>No. 2 HP saw its Unix server sales dropped 3.6...</td>\n      <td>0</td>\n      <td>29</td>\n      <td>16</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Find token_set_ratio using Levenshtein Distance via python package Fuzzywuzzy \n\nfrom fuzzywuzzy import fuzz\ndf['fuzz ratio'] = df.apply(lambda row: fuzz.ratio(row['sentence 1'].translate(str.maketrans('', '', string.punctuation)), row['sentence 2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n\ndf_dev['fuzz ratio'] = df_dev.apply(lambda row2: fuzz.ratio(row2['sentence 1'].translate(str.maketrans('', '', string.punctuation)), row2['sentence 2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n\ndf_test['fuzz ratio'] = df_test.apply(lambda row2: fuzz.ratio(row2['sentence 1'].translate(str.maketrans('', '', string.punctuation)), row2['sentence 2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:26.127331Z","iopub.execute_input":"2022-12-07T04:18:26.128720Z","iopub.status.idle":"2022-12-07T04:18:26.682271Z","shell.execute_reply.started":"2022-12-07T04:18:26.128662Z","shell.execute_reply":"2022-12-07T04:18:26.680833Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Find words that overlap\ndef getOverlap(s1,s2):\n    s2 = s2.lower()\n    s1 = s1.lower()\n    s2List = s2.split(\" \")\n    s1List = s1.split(\" \")\n    return len(list(set(s2List)&set(s1List)))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:26.684409Z","iopub.execute_input":"2022-12-07T04:18:26.684893Z","iopub.status.idle":"2022-12-07T04:18:26.692264Z","shell.execute_reply.started":"2022-12-07T04:18:26.684853Z","shell.execute_reply":"2022-12-07T04:18:26.690554Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df['overlapping words'] = df.apply(lambda row: getOverlap(row['sentence 1'], row['sentence 2']), axis = 1)\ndf_dev['overlapping words'] = df_dev.apply(lambda row: getOverlap(row['sentence 1'], row['sentence 2']), axis = 1)\ndf_test['overlapping words'] = df_test.apply(lambda row: getOverlap(row['sentence 1'], row['sentence 2']), axis = 1)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:26.694221Z","iopub.execute_input":"2022-12-07T04:18:26.694632Z","iopub.status.idle":"2022-12-07T04:18:27.160113Z","shell.execute_reply.started":"2022-12-07T04:18:26.694596Z","shell.execute_reply":"2022-12-07T04:18:27.158574Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"           id                                         sentence 1  \\\n0  train_id_0  Is it in the food supply ? \" says David Ropeik...   \n1  train_id_1  Hundreds of soldiers were involved , an appare...   \n2  train_id_2  And Sen. Michael Crapo , R-Idaho , chairman of...   \n3  train_id_3  The gunman , 26-year-old Harold Kilpatrick jnr...   \n4  train_id_4  The League of United Latin American Citizens ,...   \n\n                                          sentence 2  gold label  words_1  \\\n0  The pound also made progress against the dolla...           0       24   \n1  Avants , wearing a light brown jumpsuit , had ...           0       23   \n2  . 's Kempthorne of friend longtime a is , nomi...           0       27   \n3  \" In fact , I was physically sick several time...           0       25   \n4  No. 2 HP saw its Unix server sales dropped 3.6...           0       29   \n\n   words_2  wordcount difference  fuzz ratio  overlapping words  \n0       17                     7          41                  4  \n1       44                    21          71                 22  \n2       27                     0          42                 23  \n3       49                    24          69                 23  \n4       16                    13          30                  2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentence 1</th>\n      <th>sentence 2</th>\n      <th>gold label</th>\n      <th>words_1</th>\n      <th>words_2</th>\n      <th>wordcount difference</th>\n      <th>fuzz ratio</th>\n      <th>overlapping words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_id_0</td>\n      <td>Is it in the food supply ? \" says David Ropeik...</td>\n      <td>The pound also made progress against the dolla...</td>\n      <td>0</td>\n      <td>24</td>\n      <td>17</td>\n      <td>7</td>\n      <td>41</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_id_1</td>\n      <td>Hundreds of soldiers were involved , an appare...</td>\n      <td>Avants , wearing a light brown jumpsuit , had ...</td>\n      <td>0</td>\n      <td>23</td>\n      <td>44</td>\n      <td>21</td>\n      <td>71</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_id_2</td>\n      <td>And Sen. Michael Crapo , R-Idaho , chairman of...</td>\n      <td>. 's Kempthorne of friend longtime a is , nomi...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>27</td>\n      <td>0</td>\n      <td>42</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_id_3</td>\n      <td>The gunman , 26-year-old Harold Kilpatrick jnr...</td>\n      <td>\" In fact , I was physically sick several time...</td>\n      <td>0</td>\n      <td>25</td>\n      <td>49</td>\n      <td>24</td>\n      <td>69</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_id_4</td>\n      <td>The League of United Latin American Citizens ,...</td>\n      <td>No. 2 HP saw its Unix server sales dropped 3.6...</td>\n      <td>0</td>\n      <td>29</td>\n      <td>16</td>\n      <td>13</td>\n      <td>30</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Find difference in character count\ndf['char 1'] = df['sentence 1'].str.len()\ndf['char 2'] = df['sentence 2'].str.len()\ndf['char difference'] = abs(df['char 1'] - df['char 2'])\n\ndf_dev['char 1'] = df_dev['sentence 1'].str.len()\ndf_dev['char 2'] = df_dev['sentence 2'].str.len()\ndf_dev['char difference'] = abs(df_dev['char 1'] - df_dev['char 2'])\n\ndf_test['char 1'] = df_test['sentence 1'].str.len()\ndf_test['char 2'] = df_test['sentence 2'].str.len()\ndf_test['char difference'] = abs(df_test['char 1'] - df_test['char 2'])","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:27.162063Z","iopub.execute_input":"2022-12-07T04:18:27.163013Z","iopub.status.idle":"2022-12-07T04:18:27.202444Z","shell.execute_reply.started":"2022-12-07T04:18:27.162942Z","shell.execute_reply":"2022-12-07T04:18:27.201341Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Using package difflib to import SequenceMatcher gestalt pattern matching\nfrom difflib import SequenceMatcher\ndf['sequence matcher'] = df.apply(lambda row: SequenceMatcher(None, row['sentence 1'], row['sentence 2']).ratio(), axis = 1)\ndf_dev['sequence matcher'] = df_dev.apply(lambda row: SequenceMatcher(None, row['sentence 1'], row['sentence 2']).ratio(), axis = 1)\ndf_test['sequence matcher'] = df_test.apply(lambda row: SequenceMatcher(None, row['sentence 1'], row['sentence 2']).ratio(), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:27.204392Z","iopub.execute_input":"2022-12-07T04:18:27.205197Z","iopub.status.idle":"2022-12-07T04:18:37.721071Z","shell.execute_reply.started":"2022-12-07T04:18:27.205148Z","shell.execute_reply":"2022-12-07T04:18:37.719748Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df['Overlapping ratio'] = df['overlapping words']/df[['words_1','words_2']].min(axis=1)*2\ndf_dev['Overlapping ratio'] = df_dev['overlapping words']/df_dev[['words_1','words_2']].min(axis=1)*2\ndf_test['Overlapping ratio'] = df_test['overlapping words']/df_test[['words_1','words_2']].min(axis=1)*2","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:37.722744Z","iopub.execute_input":"2022-12-07T04:18:37.723128Z","iopub.status.idle":"2022-12-07T04:18:37.738786Z","shell.execute_reply.started":"2022-12-07T04:18:37.723093Z","shell.execute_reply":"2022-12-07T04:18:37.737442Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"nltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:37.743688Z","iopub.execute_input":"2022-12-07T04:18:37.744167Z","iopub.status.idle":"2022-12-07T04:18:37.820622Z","shell.execute_reply.started":"2022-12-07T04:18:37.744127Z","shell.execute_reply":"2022-12-07T04:18:37.819372Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import nltk \nfrom nltk.corpus import wordnet\n\ndef getSynonyms(s1, s2):\n    synonyms = []\n    antonyms = []\n    synamount = 0\n    antamount = 0\n    w1 = set(s1.translate(str.maketrans('', '', string.punctuation)).split(\" \"))\n    w2 = set(s2.translate(str.maketrans('', '', string.punctuation)).split(\" \"))\n    \n    for i in w1:\n        for syn in wordnet.synsets(i):\n            for lemma in syn.lemmas():\n                synonyms.append(lemma.name())\n                if lemma.antonyms():\n                    antonyms.append(lemma.antonyms()[0].name())\n                    \n    for synonym in set(synonyms):\n        if synonym in w2:\n            synamount += 1\n            \n    return synamount","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:37.822958Z","iopub.execute_input":"2022-12-07T04:18:37.823402Z","iopub.status.idle":"2022-12-07T04:18:37.833050Z","shell.execute_reply.started":"2022-12-07T04:18:37.823363Z","shell.execute_reply":"2022-12-07T04:18:37.831697Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df['synonyms'] = df.apply(lambda row: getSynonyms(row['sentence 1'], row['sentence 2']), axis = 1)\ndf_dev['synonyms'] = df_dev.apply(lambda row: getSynonyms(row['sentence 1'], row['sentence 2']), axis = 1)\ndf_test['synonyms'] = df_test.apply(lambda row: getSynonyms(row['sentence 1'], row['sentence 2']), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:37.835299Z","iopub.execute_input":"2022-12-07T04:18:37.835734Z","iopub.status.idle":"2022-12-07T04:18:57.816241Z","shell.execute_reply.started":"2022-12-07T04:18:37.835696Z","shell.execute_reply":"2022-12-07T04:18:57.814842Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df['synonym ratio'] = df['synonyms']/df[['words_1','words_2']].min(axis=1)*2\ndf_dev['synonym ratio'] = df_dev['synonyms']/df_dev[['words_1','words_2']].min(axis=1)*2\ndf_test['synonym ratio'] = df_test['synonyms']/df_test[['words_1','words_2']].min(axis=1)*2","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:57.818060Z","iopub.execute_input":"2022-12-07T04:18:57.819149Z","iopub.status.idle":"2022-12-07T04:18:57.836710Z","shell.execute_reply.started":"2022-12-07T04:18:57.819093Z","shell.execute_reply":"2022-12-07T04:18:57.834960Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.scatter(df['gold label'], df['fuzz ratio'])\n\nplt.xlabel('gold label')\nplt.ylabel('wordcount ratio')\n \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:57.838425Z","iopub.execute_input":"2022-12-07T04:18:57.838832Z","iopub.status.idle":"2022-12-07T04:18:58.178377Z","shell.execute_reply.started":"2022-12-07T04:18:57.838797Z","shell.execute_reply":"2022-12-07T04:18:58.176594Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#development\nimport sklearn\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n#features = ['wordcount difference', 'fuzz ratio', 'overlapping words', 'char difference', 'sequence matcher', 'wordcount ratio', 'synonyms', 'synonym ratio']\nfeatures = ['wordcount difference', 'sequence matcher', 'Overlapping ratio', 'synonym ratio']\nX_train = df[features]\nX_dev = df_dev[features]\ny_train = df['gold label']\ny_dev = df_dev['gold label']\n\nclassifier = make_pipeline(StandardScaler(), svm.SVC(kernel = 'rbf', gamma = 1, C = 1, class_weight = 'balanced'))\n#classifier = make_pipeline(StandardScaler(), svm.SVC())\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_dev)\nprint(classifier.score(X_dev, y_dev))\n\nX_test = df_test[features]\ny_test_pred = classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:58.180773Z","iopub.execute_input":"2022-12-07T04:18:58.181306Z","iopub.status.idle":"2022-12-07T04:18:59.988059Z","shell.execute_reply.started":"2022-12-07T04:18:58.181259Z","shell.execute_reply":"2022-12-07T04:18:59.986713Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"0.879\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nclf = MLPClassifier(hidden_layer_sizes=(6,5),\n                    random_state=5,\n                    verbose=True,\n                    learning_rate_init=0.01,\n                    max_iter = 200)\n\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:18:59.989982Z","iopub.execute_input":"2022-12-07T04:18:59.990377Z","iopub.status.idle":"2022-12-07T04:19:01.185239Z","shell.execute_reply.started":"2022-12-07T04:18:59.990342Z","shell.execute_reply":"2022-12-07T04:19:01.183838Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 0.65855028\nIteration 2, loss = 0.35327411\nIteration 3, loss = 0.27035301\nIteration 4, loss = 0.23697217\nIteration 5, loss = 0.22342808\nIteration 6, loss = 0.21522856\nIteration 7, loss = 0.21040345\nIteration 8, loss = 0.21157636\nIteration 9, loss = 0.21051722\nIteration 10, loss = 0.20755145\nIteration 11, loss = 0.20904243\nIteration 12, loss = 0.20483885\nIteration 13, loss = 0.20890330\nIteration 14, loss = 0.20408565\nIteration 15, loss = 0.20408603\nIteration 16, loss = 0.20148360\nIteration 17, loss = 0.20345490\nIteration 18, loss = 0.22467117\nIteration 19, loss = 0.20058009\nIteration 20, loss = 0.20068423\nIteration 21, loss = 0.20012287\nIteration 22, loss = 0.19879832\nIteration 23, loss = 0.19896368\nIteration 24, loss = 0.20093497\nIteration 25, loss = 0.19799864\nIteration 26, loss = 0.20049649\nIteration 27, loss = 0.19631211\nIteration 28, loss = 0.19616934\nIteration 29, loss = 0.19556336\nIteration 30, loss = 0.19520255\nIteration 31, loss = 0.19760455\nIteration 32, loss = 0.20787059\nIteration 33, loss = 0.22539598\nIteration 34, loss = 0.19757170\nIteration 35, loss = 0.19474227\nIteration 36, loss = 0.19331293\nIteration 37, loss = 0.19221853\nIteration 38, loss = 0.19157267\nIteration 39, loss = 0.19207840\nIteration 40, loss = 0.19916698\nIteration 41, loss = 0.19056392\nIteration 42, loss = 0.19080276\nIteration 43, loss = 0.19038225\nIteration 44, loss = 0.19059943\nIteration 45, loss = 0.19199778\nIteration 46, loss = 0.26964231\nIteration 47, loss = 0.21998160\nIteration 48, loss = 0.20960360\nIteration 49, loss = 0.20755834\nIteration 50, loss = 0.19813203\nIteration 51, loss = 0.19720041\nIteration 52, loss = 0.19889543\nIteration 53, loss = 0.25130539\nIteration 54, loss = 0.21415193\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"MLPClassifier(hidden_layer_sizes=(6, 5), learning_rate_init=0.01,\n              random_state=5, verbose=True)"},"metadata":{}}]},{"cell_type":"code","source":"#Calculate classifier score\nypred=clf.predict(X_dev)\nprint(clf.score(X_dev, y_dev))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:19:01.187089Z","iopub.execute_input":"2022-12-07T04:19:01.188630Z","iopub.status.idle":"2022-12-07T04:19:01.207408Z","shell.execute_reply.started":"2022-12-07T04:19:01.188565Z","shell.execute_reply":"2022-12-07T04:19:01.205946Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"0.897\n","output_type":"stream"}]},{"cell_type":"code","source":"#Calculate f1 Score\nf1 = sklearn.metrics.f1_score(y_dev, ypred, average='weighted')\nprint(f1)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:19:01.209744Z","iopub.execute_input":"2022-12-07T04:19:01.211291Z","iopub.status.idle":"2022-12-07T04:19:01.226656Z","shell.execute_reply.started":"2022-12-07T04:19:01.211216Z","shell.execute_reply":"2022-12-07T04:19:01.224855Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"0.8991117498537455\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test for Overfitting by using model on training and development data\n\n# Evaluate the model on the training set\ntrain_acc = clf.score(X_train, y_train)\nprint(\"train_acc = \" + str(train_acc))\n\n# Evaluate the model on the development set\ndev_acc = clf.score(X_dev, y_dev)\nprint(\"dev_acc = \" + str(dev_acc))\n\n#Seeing how both numbers are very similar, this is a sign that the model is not overfitting ","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:19:01.228990Z","iopub.execute_input":"2022-12-07T04:19:01.230157Z","iopub.status.idle":"2022-12-07T04:19:01.271200Z","shell.execute_reply.started":"2022-12-07T04:19:01.230082Z","shell.execute_reply":"2022-12-07T04:19:01.267327Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"train_acc = 0.905653121394693\ndev_acc = 0.897\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Split the data into 5 folds\nfolds = 5\n\n# Use cross-validation to evaluate the models on the data\ndev_score = cross_val_score(clf, X_dev, y_dev)\ntrain_score = cross_val_score(clf, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:22:33.677122Z","iopub.execute_input":"2022-12-07T04:22:33.677628Z","iopub.status.idle":"2022-12-07T04:22:47.738378Z","shell.execute_reply.started":"2022-12-07T04:22:33.677587Z","shell.execute_reply":"2022-12-07T04:22:47.737141Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 0.86076609\nIteration 2, loss = 0.53266659\nIteration 3, loss = 0.42165692\nIteration 4, loss = 0.35622400\nIteration 5, loss = 0.31223322\nIteration 6, loss = 0.28411696\nIteration 7, loss = 0.26375200\nIteration 8, loss = 0.25119219\nIteration 9, loss = 0.24195520\nIteration 10, loss = 0.23896512\nIteration 11, loss = 0.23490220\nIteration 12, loss = 0.23037261\nIteration 13, loss = 0.22816744\nIteration 14, loss = 0.22627713\nIteration 15, loss = 0.22647060\nIteration 16, loss = 0.22458732\nIteration 17, loss = 0.22117561\nIteration 18, loss = 0.22007919\nIteration 19, loss = 0.21893535\nIteration 20, loss = 0.21789368\nIteration 21, loss = 0.21785915\nIteration 22, loss = 0.21769285\nIteration 23, loss = 0.21542488\nIteration 24, loss = 0.21540489\nIteration 25, loss = 0.21361199\nIteration 26, loss = 0.21630987\nIteration 27, loss = 0.21544792\nIteration 28, loss = 0.21450559\nIteration 29, loss = 0.21651216\nIteration 30, loss = 0.21777531\nIteration 31, loss = 0.21454441\nIteration 32, loss = 0.21230180\nIteration 33, loss = 0.21227765\nIteration 34, loss = 0.21303816\nIteration 35, loss = 0.21112044\nIteration 36, loss = 0.21152574\nIteration 37, loss = 0.20976943\nIteration 38, loss = 0.21127883\nIteration 39, loss = 0.20939082\nIteration 40, loss = 0.20880882\nIteration 41, loss = 0.21111810\nIteration 42, loss = 0.20989890\nIteration 43, loss = 0.20752661\nIteration 44, loss = 0.20596726\nIteration 45, loss = 0.20580202\nIteration 46, loss = 0.20515512\nIteration 47, loss = 0.20640352\nIteration 48, loss = 0.20671633\nIteration 49, loss = 0.20550450\nIteration 50, loss = 0.20528559\nIteration 51, loss = 0.20367770\nIteration 52, loss = 0.20385132\nIteration 53, loss = 0.20420861\nIteration 54, loss = 0.20756294\nIteration 55, loss = 0.20470828\nIteration 56, loss = 0.20360584\nIteration 57, loss = 0.20376449\nIteration 58, loss = 0.20240684\nIteration 59, loss = 0.20340140\nIteration 60, loss = 0.20707549\nIteration 61, loss = 0.20617891\nIteration 62, loss = 0.20490356\nIteration 63, loss = 0.20260297\nIteration 64, loss = 0.20350115\nIteration 65, loss = 0.20194106\nIteration 66, loss = 0.20247842\nIteration 67, loss = 0.20163343\nIteration 68, loss = 0.20164519\nIteration 69, loss = 0.20247729\nIteration 70, loss = 0.20212900\nIteration 71, loss = 0.20073135\nIteration 72, loss = 0.20081829\nIteration 73, loss = 0.20120582\nIteration 74, loss = 0.20236116\nIteration 75, loss = 0.20091338\nIteration 76, loss = 0.20086095\nIteration 77, loss = 0.20081832\nIteration 78, loss = 0.20134340\nIteration 79, loss = 0.20004485\nIteration 80, loss = 0.20059083\nIteration 81, loss = 0.20225188\nIteration 82, loss = 0.20035066\nIteration 83, loss = 0.20013213\nIteration 84, loss = 0.20114643\nIteration 85, loss = 0.19993220\nIteration 86, loss = 0.19954632\nIteration 87, loss = 0.20072249\nIteration 88, loss = 0.19950603\nIteration 89, loss = 0.19910901\nIteration 90, loss = 0.19922755\nIteration 91, loss = 0.19886750\nIteration 92, loss = 0.19838181\nIteration 93, loss = 0.19967508\nIteration 94, loss = 0.19986375\nIteration 95, loss = 0.20080045\nIteration 96, loss = 0.20202970\nIteration 97, loss = 0.19976967\nIteration 98, loss = 0.19918092\nIteration 99, loss = 0.19836294\nIteration 100, loss = 0.20114815\nIteration 101, loss = 0.19722916\nIteration 102, loss = 0.20269295\nIteration 103, loss = 0.19941517\nIteration 104, loss = 0.20076385\nIteration 105, loss = 0.19972858\nIteration 106, loss = 0.19966617\nIteration 107, loss = 0.19816703\nIteration 108, loss = 0.19830786\nIteration 109, loss = 0.19907737\nIteration 110, loss = 0.19832272\nIteration 111, loss = 0.19814664\nIteration 112, loss = 0.19918326\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.87840684\nIteration 2, loss = 0.53440444\nIteration 3, loss = 0.41962665\nIteration 4, loss = 0.35347970\nIteration 5, loss = 0.30910258\nIteration 6, loss = 0.27965533\nIteration 7, loss = 0.25781060\nIteration 8, loss = 0.24421650\nIteration 9, loss = 0.23340823\nIteration 10, loss = 0.22950955\nIteration 11, loss = 0.22553584\nIteration 12, loss = 0.22087275\nIteration 13, loss = 0.21925029\nIteration 14, loss = 0.21697018\nIteration 15, loss = 0.21643462\nIteration 16, loss = 0.21888121\nIteration 17, loss = 0.21691900\nIteration 18, loss = 0.21497630\nIteration 19, loss = 0.21237670\nIteration 20, loss = 0.21286539\nIteration 21, loss = 0.21472230\nIteration 22, loss = 0.21340887\nIteration 23, loss = 0.21197005\nIteration 24, loss = 0.21165037\nIteration 25, loss = 0.21078346\nIteration 26, loss = 0.21035229\nIteration 27, loss = 0.20940757\nIteration 28, loss = 0.21213601\nIteration 29, loss = 0.21311269\nIteration 30, loss = 0.21163650\nIteration 31, loss = 0.20941584\nIteration 32, loss = 0.20881442\nIteration 33, loss = 0.20834272\nIteration 34, loss = 0.20839741\nIteration 35, loss = 0.20794013\nIteration 36, loss = 0.20834047\nIteration 37, loss = 0.20777327\nIteration 38, loss = 0.20669561\nIteration 39, loss = 0.21047718\nIteration 40, loss = 0.20633225\nIteration 41, loss = 0.20656446\nIteration 42, loss = 0.20713660\nIteration 43, loss = 0.20599553\nIteration 44, loss = 0.20664637\nIteration 45, loss = 0.20606511\nIteration 46, loss = 0.20460882\nIteration 47, loss = 0.20558680\nIteration 48, loss = 0.20625609\nIteration 49, loss = 0.20499743\nIteration 50, loss = 0.20509330\nIteration 51, loss = 0.20562139\nIteration 52, loss = 0.20636580\nIteration 53, loss = 0.20622179\nIteration 54, loss = 0.20665865\nIteration 55, loss = 0.20461808\nIteration 56, loss = 0.20406833\nIteration 57, loss = 0.20450531\nIteration 58, loss = 0.20247451\nIteration 59, loss = 0.20278603\nIteration 60, loss = 0.20394752\nIteration 61, loss = 0.20375539\nIteration 62, loss = 0.20332121\nIteration 63, loss = 0.20308149\nIteration 64, loss = 0.20158394\nIteration 65, loss = 0.20287347\nIteration 66, loss = 0.20286905\nIteration 67, loss = 0.20309325\nIteration 68, loss = 0.20245892\nIteration 69, loss = 0.20283528\nIteration 70, loss = 0.20176443\nIteration 71, loss = 0.20227444\nIteration 72, loss = 0.20156344\nIteration 73, loss = 0.20272340\nIteration 74, loss = 0.20258546\nIteration 75, loss = 0.20092174\nIteration 76, loss = 0.20197221\nIteration 77, loss = 0.20463039\nIteration 78, loss = 0.20588769\nIteration 79, loss = 0.20049634\nIteration 80, loss = 0.20113529\nIteration 81, loss = 0.20023742\nIteration 82, loss = 0.20171679\nIteration 83, loss = 0.19975889\nIteration 84, loss = 0.20012027\nIteration 85, loss = 0.19982483\nIteration 86, loss = 0.20237732\nIteration 87, loss = 0.20037016\nIteration 88, loss = 0.19879357\nIteration 89, loss = 0.20036413\nIteration 90, loss = 0.19970809\nIteration 91, loss = 0.19970954\nIteration 92, loss = 0.20029692\nIteration 93, loss = 0.20106230\nIteration 94, loss = 0.20102176\nIteration 95, loss = 0.19983179\nIteration 96, loss = 0.20406970\nIteration 97, loss = 0.19935743\nIteration 98, loss = 0.19996664\nIteration 99, loss = 0.19841219\nIteration 100, loss = 0.20089086\nIteration 101, loss = 0.20031551\nIteration 102, loss = 0.20257288\nIteration 103, loss = 0.19917300\nIteration 104, loss = 0.19931146\nIteration 105, loss = 0.19951581\nIteration 106, loss = 0.19807825\nIteration 107, loss = 0.19925808\nIteration 108, loss = 0.19759377\nIteration 109, loss = 0.19829027\nIteration 110, loss = 0.19725441\nIteration 111, loss = 0.19875710\nIteration 112, loss = 0.19782700\nIteration 113, loss = 0.19892597\nIteration 114, loss = 0.19726805\nIteration 115, loss = 0.19798165\nIteration 116, loss = 0.19732072\nIteration 117, loss = 0.19652192\nIteration 118, loss = 0.19757280\nIteration 119, loss = 0.19768935\nIteration 120, loss = 0.19784527\nIteration 121, loss = 0.19689582\nIteration 122, loss = 0.19820293\nIteration 123, loss = 0.20180326\nIteration 124, loss = 0.19875042\nIteration 125, loss = 0.19673691\nIteration 126, loss = 0.19642739\nIteration 127, loss = 0.19979034\nIteration 128, loss = 0.21042336\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.85963321\nIteration 2, loss = 0.53251315\nIteration 3, loss = 0.42167048\nIteration 4, loss = 0.35121677\nIteration 5, loss = 0.30663405\nIteration 6, loss = 0.27808816\nIteration 7, loss = 0.25705854\nIteration 8, loss = 0.24430627\nIteration 9, loss = 0.23617060\nIteration 10, loss = 0.23108612\nIteration 11, loss = 0.22758013\nIteration 12, loss = 0.22522791\nIteration 13, loss = 0.22515261\nIteration 14, loss = 0.22522919\nIteration 15, loss = 0.22316531\nIteration 16, loss = 0.21992409\nIteration 17, loss = 0.22098116\nIteration 18, loss = 0.22237464\nIteration 19, loss = 0.21743815\nIteration 20, loss = 0.21770239\nIteration 21, loss = 0.21745833\nIteration 22, loss = 0.21795150\nIteration 23, loss = 0.21720792\nIteration 24, loss = 0.21655108\nIteration 25, loss = 0.21594293\nIteration 26, loss = 0.21545076\nIteration 27, loss = 0.21453928\nIteration 28, loss = 0.21749844\nIteration 29, loss = 0.21994702\nIteration 30, loss = 0.21633527\nIteration 31, loss = 0.21361684\nIteration 32, loss = 0.21517667\nIteration 33, loss = 0.21292982\nIteration 34, loss = 0.21321028\nIteration 35, loss = 0.21284994\nIteration 36, loss = 0.21241265\nIteration 37, loss = 0.21283724\nIteration 38, loss = 0.21055567\nIteration 39, loss = 0.21566201\nIteration 40, loss = 0.21240530\nIteration 41, loss = 0.20915523\nIteration 42, loss = 0.20872772\nIteration 43, loss = 0.20797966\nIteration 44, loss = 0.20734140\nIteration 45, loss = 0.20786184\nIteration 46, loss = 0.20684159\nIteration 47, loss = 0.21228263\nIteration 48, loss = 0.21009126\nIteration 49, loss = 0.20663894\nIteration 50, loss = 0.20930197\nIteration 51, loss = 0.20664047\nIteration 52, loss = 0.20635284\nIteration 53, loss = 0.20572571\nIteration 54, loss = 0.20709091\nIteration 55, loss = 0.20577871\nIteration 56, loss = 0.20649758\nIteration 57, loss = 0.20728553\nIteration 58, loss = 0.20551127\nIteration 59, loss = 0.20497079\nIteration 60, loss = 0.20676918\nIteration 61, loss = 0.20554070\nIteration 62, loss = 0.20476321\nIteration 63, loss = 0.20470495\nIteration 64, loss = 0.20372892\nIteration 65, loss = 0.20380201\nIteration 66, loss = 0.20356575\nIteration 67, loss = 0.20866845\nIteration 68, loss = 0.20557774\nIteration 69, loss = 0.20491035\nIteration 70, loss = 0.20476365\nIteration 71, loss = 0.20311514\nIteration 72, loss = 0.20472423\nIteration 73, loss = 0.20352793\nIteration 74, loss = 0.20289224\nIteration 75, loss = 0.20187720\nIteration 76, loss = 0.20192384\nIteration 77, loss = 0.20566356\nIteration 78, loss = 0.20399332\nIteration 79, loss = 0.20099738\nIteration 80, loss = 0.20182372\nIteration 81, loss = 0.20112718\nIteration 82, loss = 0.20322438\nIteration 83, loss = 0.20066991\nIteration 84, loss = 0.20378663\nIteration 85, loss = 0.20137883\nIteration 86, loss = 0.20072419\nIteration 87, loss = 0.20222490\nIteration 88, loss = 0.20036105\nIteration 89, loss = 0.20037913\nIteration 90, loss = 0.20118372\nIteration 91, loss = 0.20022092\nIteration 92, loss = 0.20248745\nIteration 93, loss = 0.20193729\nIteration 94, loss = 0.20042881\nIteration 95, loss = 0.20582929\nIteration 96, loss = 0.20081772\nIteration 97, loss = 0.19960812\nIteration 98, loss = 0.19994800\nIteration 99, loss = 0.19867653\nIteration 100, loss = 0.20061418\nIteration 101, loss = 0.20052179\nIteration 102, loss = 0.20016086\nIteration 103, loss = 0.19937222\nIteration 104, loss = 0.19981437\nIteration 105, loss = 0.19832574\nIteration 106, loss = 0.19880492\nIteration 107, loss = 0.19860922\nIteration 108, loss = 0.19862579\nIteration 109, loss = 0.19823997\nIteration 110, loss = 0.19800089\nIteration 111, loss = 0.19963436\nIteration 112, loss = 0.19931827\nIteration 113, loss = 0.19881635\nIteration 114, loss = 0.19775810\nIteration 115, loss = 0.19891649\nIteration 116, loss = 0.19982050\nIteration 117, loss = 0.19847401\nIteration 118, loss = 0.19766202\nIteration 119, loss = 0.20007136\nIteration 120, loss = 0.19844629\nIteration 121, loss = 0.19810175\nIteration 122, loss = 0.19736162\nIteration 123, loss = 0.19871358\nIteration 124, loss = 0.19789226\nIteration 125, loss = 0.19773024\nIteration 126, loss = 0.19735124\nIteration 127, loss = 0.19752555\nIteration 128, loss = 0.20063637\nIteration 129, loss = 0.19931194\nIteration 130, loss = 0.19932757\nIteration 131, loss = 0.19843965\nIteration 132, loss = 0.19854356\nIteration 133, loss = 0.19748196\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.88459747\nIteration 2, loss = 0.53820472\nIteration 3, loss = 0.42093332\nIteration 4, loss = 0.35492312\nIteration 5, loss = 0.31249931\nIteration 6, loss = 0.28388422\nIteration 7, loss = 0.26252880\nIteration 8, loss = 0.24892377\nIteration 9, loss = 0.24015167\nIteration 10, loss = 0.23508979\nIteration 11, loss = 0.23167607\nIteration 12, loss = 0.22780926\nIteration 13, loss = 0.22593002\nIteration 14, loss = 0.22688140\nIteration 15, loss = 0.22910621\nIteration 16, loss = 0.22304325\nIteration 17, loss = 0.22175936\nIteration 18, loss = 0.22190051\nIteration 19, loss = 0.21989058\nIteration 20, loss = 0.22080871\nIteration 21, loss = 0.22038249\nIteration 22, loss = 0.21941540\nIteration 23, loss = 0.21812015\nIteration 24, loss = 0.21660220\nIteration 25, loss = 0.21582530\nIteration 26, loss = 0.21719535\nIteration 27, loss = 0.21448519\nIteration 28, loss = 0.21706554\nIteration 29, loss = 0.21779834\nIteration 30, loss = 0.21596273\nIteration 31, loss = 0.21391533\nIteration 32, loss = 0.21335427\nIteration 33, loss = 0.21326936\nIteration 34, loss = 0.21502023\nIteration 35, loss = 0.21455909\nIteration 36, loss = 0.21243877\nIteration 37, loss = 0.21273920\nIteration 38, loss = 0.21214034\nIteration 39, loss = 0.21458953\nIteration 40, loss = 0.21448183\nIteration 41, loss = 0.21176676\nIteration 42, loss = 0.21107751\nIteration 43, loss = 0.21108849\nIteration 44, loss = 0.20983756\nIteration 45, loss = 0.21114358\nIteration 46, loss = 0.21194422\nIteration 47, loss = 0.21326263\nIteration 48, loss = 0.21096230\nIteration 49, loss = 0.20951174\nIteration 50, loss = 0.21608200\nIteration 51, loss = 0.21347026\nIteration 52, loss = 0.20947965\nIteration 53, loss = 0.20956464\nIteration 54, loss = 0.20785152\nIteration 55, loss = 0.21133290\nIteration 56, loss = 0.21062531\nIteration 57, loss = 0.21284667\nIteration 58, loss = 0.21008290\nIteration 59, loss = 0.20885010\nIteration 60, loss = 0.20878699\nIteration 61, loss = 0.20699438\nIteration 62, loss = 0.20740595\nIteration 63, loss = 0.20654322\nIteration 64, loss = 0.20883920\nIteration 65, loss = 0.20736241\nIteration 66, loss = 0.20816053\nIteration 67, loss = 0.20815891\nIteration 68, loss = 0.20836361\nIteration 69, loss = 0.20661904\nIteration 70, loss = 0.20756838\nIteration 71, loss = 0.20499571\nIteration 72, loss = 0.20642536\nIteration 73, loss = 0.20516556\nIteration 74, loss = 0.20613842\nIteration 75, loss = 0.20581226\nIteration 76, loss = 0.20551736\nIteration 77, loss = 0.20485792\nIteration 78, loss = 0.20469751\nIteration 79, loss = 0.20679841\nIteration 80, loss = 0.20568341\nIteration 81, loss = 0.20543909\nIteration 82, loss = 0.20698486\nIteration 83, loss = 0.20390226\nIteration 84, loss = 0.20565895\nIteration 85, loss = 0.20464478\nIteration 86, loss = 0.20573708\nIteration 87, loss = 0.20428528\nIteration 88, loss = 0.20537429\nIteration 89, loss = 0.20349831\nIteration 90, loss = 0.20520182\nIteration 91, loss = 0.20444723\nIteration 92, loss = 0.20347224\nIteration 93, loss = 0.20579506\nIteration 94, loss = 0.20607372\nIteration 95, loss = 0.20432051\nIteration 96, loss = 0.20287882\nIteration 97, loss = 0.20400793\nIteration 98, loss = 0.20329340\nIteration 99, loss = 0.20251060\nIteration 100, loss = 0.20344351\nIteration 101, loss = 0.20293050\nIteration 102, loss = 0.20278724\nIteration 103, loss = 0.20290758\nIteration 104, loss = 0.20292165\nIteration 105, loss = 0.20276958\nIteration 106, loss = 0.20296003\nIteration 107, loss = 0.20273128\nIteration 108, loss = 0.20351723\nIteration 109, loss = 0.20192859\nIteration 110, loss = 0.20455513\nIteration 111, loss = 0.20452836\nIteration 112, loss = 0.20650148\nIteration 113, loss = 0.20431266\nIteration 114, loss = 0.20430668\nIteration 115, loss = 0.20925366\nIteration 116, loss = 0.20888018\nIteration 117, loss = 0.20518707\nIteration 118, loss = 0.20281945\nIteration 119, loss = 0.20460581\nIteration 120, loss = 0.20342568\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.87043867\nIteration 2, loss = 0.53294851\nIteration 3, loss = 0.42314630\nIteration 4, loss = 0.35343760\nIteration 5, loss = 0.31293294\nIteration 6, loss = 0.28519582\nIteration 7, loss = 0.26410475\nIteration 8, loss = 0.25198177\nIteration 9, loss = 0.24368675\nIteration 10, loss = 0.24303470\nIteration 11, loss = 0.23835224\nIteration 12, loss = 0.23340437\nIteration 13, loss = 0.22938338\nIteration 14, loss = 0.22900796\nIteration 15, loss = 0.22887960\nIteration 16, loss = 0.22512576\nIteration 17, loss = 0.22523525\nIteration 18, loss = 0.22756572\nIteration 19, loss = 0.22278067\nIteration 20, loss = 0.22217548\nIteration 21, loss = 0.22131297\nIteration 22, loss = 0.22226241\nIteration 23, loss = 0.22303178\nIteration 24, loss = 0.21952670\nIteration 25, loss = 0.22032990\nIteration 26, loss = 0.21949557\nIteration 27, loss = 0.21884623\nIteration 28, loss = 0.22177756\nIteration 29, loss = 0.22061136\nIteration 30, loss = 0.22460723\nIteration 31, loss = 0.22283851\nIteration 32, loss = 0.21957800\nIteration 33, loss = 0.21839238\nIteration 34, loss = 0.22089947\nIteration 35, loss = 0.22215358\nIteration 36, loss = 0.21746499\nIteration 37, loss = 0.22029100\nIteration 38, loss = 0.21908525\nIteration 39, loss = 0.21700924\nIteration 40, loss = 0.21931634\nIteration 41, loss = 0.21660612\nIteration 42, loss = 0.21631353\nIteration 43, loss = 0.21754822\nIteration 44, loss = 0.21840510\nIteration 45, loss = 0.21818181\nIteration 46, loss = 0.21816642\nIteration 47, loss = 0.21697615\nIteration 48, loss = 0.21560909\nIteration 49, loss = 0.21666947\nIteration 50, loss = 0.22293879\nIteration 51, loss = 0.22035815\nIteration 52, loss = 0.21571364\nIteration 53, loss = 0.21404294\nIteration 54, loss = 0.21397467\nIteration 55, loss = 0.21548357\nIteration 56, loss = 0.21502265\nIteration 57, loss = 0.21713498\nIteration 58, loss = 0.21617923\nIteration 59, loss = 0.21401584\nIteration 60, loss = 0.21410041\nIteration 61, loss = 0.21361383\nIteration 62, loss = 0.21394318\nIteration 63, loss = 0.21465342\nIteration 64, loss = 0.21590394\nIteration 65, loss = 0.21421645\nIteration 66, loss = 0.21577185\nIteration 67, loss = 0.21639714\nIteration 68, loss = 0.21588519\nIteration 69, loss = 0.21364602\nIteration 70, loss = 0.21328533\nIteration 71, loss = 0.21164627\nIteration 72, loss = 0.21748504\nIteration 73, loss = 0.21287171\nIteration 74, loss = 0.21235527\nIteration 75, loss = 0.21196365\nIteration 76, loss = 0.21229206\nIteration 77, loss = 0.21116055\nIteration 78, loss = 0.21066289\nIteration 79, loss = 0.21188251\nIteration 80, loss = 0.21039673\nIteration 81, loss = 0.21068663\nIteration 82, loss = 0.21388906\nIteration 83, loss = 0.21003315\nIteration 84, loss = 0.21242570\nIteration 85, loss = 0.21049557\nIteration 86, loss = 0.21023732\nIteration 87, loss = 0.21031632\nIteration 88, loss = 0.21389592\nIteration 89, loss = 0.20855230\nIteration 90, loss = 0.20884014\nIteration 91, loss = 0.20723591\nIteration 92, loss = 0.20886026\nIteration 93, loss = 0.20779487\nIteration 94, loss = 0.20960480\nIteration 95, loss = 0.20849045\nIteration 96, loss = 0.21099306\nIteration 97, loss = 0.21137771\nIteration 98, loss = 0.20745693\nIteration 99, loss = 0.20673091\nIteration 100, loss = 0.20545716\nIteration 101, loss = 0.20438730\nIteration 102, loss = 0.20538105\nIteration 103, loss = 0.20529444\nIteration 104, loss = 0.20407571\nIteration 105, loss = 0.20344801\nIteration 106, loss = 0.20318511\nIteration 107, loss = 0.20375443\nIteration 108, loss = 0.20263135\nIteration 109, loss = 0.20315128\nIteration 110, loss = 0.21192408\nIteration 111, loss = 0.20419640\nIteration 112, loss = 0.20447210\nIteration 113, loss = 0.20341636\nIteration 114, loss = 0.20228179\nIteration 115, loss = 0.20436686\nIteration 116, loss = 0.20449342\nIteration 117, loss = 0.20652982\nIteration 118, loss = 0.20474146\nIteration 119, loss = 0.20595555\nIteration 120, loss = 0.20661861\nIteration 121, loss = 0.20219946\nIteration 122, loss = 0.20144234\nIteration 123, loss = 0.20124277\nIteration 124, loss = 0.20037834\nIteration 125, loss = 0.20251890\nIteration 126, loss = 0.20103024\nIteration 127, loss = 0.20079464\nIteration 128, loss = 0.20188226\nIteration 129, loss = 0.20015314\nIteration 130, loss = 0.20309052\nIteration 131, loss = 0.20290316\nIteration 132, loss = 0.20213209\nIteration 133, loss = 0.20184469\nIteration 134, loss = 0.20163731\nIteration 135, loss = 0.19974936\nIteration 136, loss = 0.20235186\nIteration 137, loss = 0.20261625\nIteration 138, loss = 0.20270361\nIteration 139, loss = 0.19946758\nIteration 140, loss = 0.20077722\nIteration 141, loss = 0.20107073\nIteration 142, loss = 0.20126555\nIteration 143, loss = 0.20028194\nIteration 144, loss = 0.20006348\nIteration 145, loss = 0.20294343\nIteration 146, loss = 0.20122883\nIteration 147, loss = 0.20075168\nIteration 148, loss = 0.20944139\nIteration 149, loss = 0.20819788\nIteration 150, loss = 0.20320584\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.70912443\nIteration 2, loss = 0.39102601\nIteration 3, loss = 0.29514107\nIteration 4, loss = 0.24985021\nIteration 5, loss = 0.23293669\nIteration 6, loss = 0.22270117\nIteration 7, loss = 0.21879803\nIteration 8, loss = 0.21667943\nIteration 9, loss = 0.21347312\nIteration 10, loss = 0.20986544\nIteration 11, loss = 0.20904335\nIteration 12, loss = 0.20817337\nIteration 13, loss = 0.20560106\nIteration 14, loss = 0.20656157\nIteration 15, loss = 0.20980113\nIteration 16, loss = 0.20336998\nIteration 17, loss = 0.20338443\nIteration 18, loss = 0.20370691\nIteration 19, loss = 0.20512649\nIteration 20, loss = 0.20128263\nIteration 21, loss = 0.20173659\nIteration 22, loss = 0.20111445\nIteration 23, loss = 0.20037848\nIteration 24, loss = 0.20258317\nIteration 25, loss = 0.20401209\nIteration 26, loss = 0.20156616\nIteration 27, loss = 0.20095623\nIteration 28, loss = 0.19817000\nIteration 29, loss = 0.19762276\nIteration 30, loss = 0.19805750\nIteration 31, loss = 0.19846418\nIteration 32, loss = 0.19836143\nIteration 33, loss = 0.19710261\nIteration 34, loss = 0.19902201\nIteration 35, loss = 0.19671843\nIteration 36, loss = 0.19444892\nIteration 37, loss = 0.19483898\nIteration 38, loss = 0.19375126\nIteration 39, loss = 0.19775880\nIteration 40, loss = 0.19524592\nIteration 41, loss = 0.19873563\nIteration 42, loss = 0.19414351\nIteration 43, loss = 0.19297258\nIteration 44, loss = 0.19560694\nIteration 45, loss = 0.19201446\nIteration 46, loss = 0.19304898\nIteration 47, loss = 0.19289815\nIteration 48, loss = 0.19226801\nIteration 49, loss = 0.19130222\nIteration 50, loss = 0.19003525\nIteration 51, loss = 0.19209164\nIteration 52, loss = 0.19018368\nIteration 53, loss = 0.19132723\nIteration 54, loss = 0.19023126\nIteration 55, loss = 0.19165452\nIteration 56, loss = 0.19095671\nIteration 57, loss = 0.18924644\nIteration 58, loss = 0.19049187\nIteration 59, loss = 0.18880057\nIteration 60, loss = 0.18875761\nIteration 61, loss = 0.18921126\nIteration 62, loss = 0.19507157\nIteration 63, loss = 0.19222865\nIteration 64, loss = 0.19303702\nIteration 65, loss = 0.18858678\nIteration 66, loss = 0.18823021\nIteration 67, loss = 0.18737407\nIteration 68, loss = 0.18915045\nIteration 69, loss = 0.18794832\nIteration 70, loss = 0.18982137\nIteration 71, loss = 0.18890123\nIteration 72, loss = 0.18710645\nIteration 73, loss = 0.18790406\nIteration 74, loss = 0.18697286\nIteration 75, loss = 0.19085305\nIteration 76, loss = 0.18666901\nIteration 77, loss = 0.18740435\nIteration 78, loss = 0.18694501\nIteration 79, loss = 0.18711514\nIteration 80, loss = 0.19011105\nIteration 81, loss = 0.18588929\nIteration 82, loss = 0.18579540\nIteration 83, loss = 0.18742481\nIteration 84, loss = 0.18749692\nIteration 85, loss = 0.18939280\nIteration 86, loss = 0.19033001\nIteration 87, loss = 0.18628657\nIteration 88, loss = 0.18744965\nIteration 89, loss = 0.18566301\nIteration 90, loss = 0.18576041\nIteration 91, loss = 0.19054490\nIteration 92, loss = 0.18536478\nIteration 93, loss = 0.18551684\nIteration 94, loss = 0.18698923\nIteration 95, loss = 0.18471089\nIteration 96, loss = 0.18547402\nIteration 97, loss = 0.19004850\nIteration 98, loss = 0.18670230\nIteration 99, loss = 0.18661840\nIteration 100, loss = 0.18552865\nIteration 101, loss = 0.18399435\nIteration 102, loss = 0.18497661\nIteration 103, loss = 0.18776564\nIteration 104, loss = 0.18546025\nIteration 105, loss = 0.18619065\nIteration 106, loss = 0.18494734\nIteration 107, loss = 0.18885545\nIteration 108, loss = 0.18641905\nIteration 109, loss = 0.18661811\nIteration 110, loss = 0.18432240\nIteration 111, loss = 0.18447335\nIteration 112, loss = 0.18503628\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.73086272\nIteration 2, loss = 0.38910739\nIteration 3, loss = 0.29693324\nIteration 4, loss = 0.25534665\nIteration 5, loss = 0.23133624\nIteration 6, loss = 0.22306413\nIteration 7, loss = 0.21670681\nIteration 8, loss = 0.21413400\nIteration 9, loss = 0.21109170\nIteration 10, loss = 0.20994103\nIteration 11, loss = 0.20816937\nIteration 12, loss = 0.20805323\nIteration 13, loss = 0.20685889\nIteration 14, loss = 0.20794422\nIteration 15, loss = 0.20508141\nIteration 16, loss = 0.20460983\nIteration 17, loss = 0.20388436\nIteration 18, loss = 0.20481482\nIteration 19, loss = 0.20098023\nIteration 20, loss = 0.20141773\nIteration 21, loss = 0.20352822\nIteration 22, loss = 0.20320007\nIteration 23, loss = 0.19987109\nIteration 24, loss = 0.20198119\nIteration 25, loss = 0.19820896\nIteration 26, loss = 0.19610903\nIteration 27, loss = 0.19713456\nIteration 28, loss = 0.19554034\nIteration 29, loss = 0.19435332\nIteration 30, loss = 0.19951811\nIteration 31, loss = 0.19502396\nIteration 32, loss = 0.19918841\nIteration 33, loss = 0.19405302\nIteration 34, loss = 0.19245703\nIteration 35, loss = 0.19595273\nIteration 36, loss = 0.19389031\nIteration 37, loss = 0.19390948\nIteration 38, loss = 0.19529792\nIteration 39, loss = 0.19524930\nIteration 40, loss = 0.19329696\nIteration 41, loss = 0.19383242\nIteration 42, loss = 0.19093492\nIteration 43, loss = 0.19134329\nIteration 44, loss = 0.19062995\nIteration 45, loss = 0.19285715\nIteration 46, loss = 0.19283958\nIteration 47, loss = 0.19160177\nIteration 48, loss = 0.19223900\nIteration 49, loss = 0.19011813\nIteration 50, loss = 0.19337741\nIteration 51, loss = 0.19733202\nIteration 52, loss = 0.19123863\nIteration 53, loss = 0.19188033\nIteration 54, loss = 0.18958116\nIteration 55, loss = 0.19202544\nIteration 56, loss = 0.18989322\nIteration 57, loss = 0.18922230\nIteration 58, loss = 0.19170175\nIteration 59, loss = 0.18974111\nIteration 60, loss = 0.19818517\nIteration 61, loss = 0.19070910\nIteration 62, loss = 0.18931546\nIteration 63, loss = 0.18984778\nIteration 64, loss = 0.18886289\nIteration 65, loss = 0.18965616\nIteration 66, loss = 0.19095378\nIteration 67, loss = 0.18853313\nIteration 68, loss = 0.19014560\nIteration 69, loss = 0.18842885\nIteration 70, loss = 0.18866817\nIteration 71, loss = 0.18977561\nIteration 72, loss = 0.19021050\nIteration 73, loss = 0.18899075\nIteration 74, loss = 0.19184000\nIteration 75, loss = 0.18819003\nIteration 76, loss = 0.18795343\nIteration 77, loss = 0.19160768\nIteration 78, loss = 0.18797795\nIteration 79, loss = 0.19088704\nIteration 80, loss = 0.18966453\nIteration 81, loss = 0.19084507\nIteration 82, loss = 0.18862386\nIteration 83, loss = 0.18782179\nIteration 84, loss = 0.18726877\nIteration 85, loss = 0.18869652\nIteration 86, loss = 0.18778036\nIteration 87, loss = 0.18780278\nIteration 88, loss = 0.18783958\nIteration 89, loss = 0.18765375\nIteration 90, loss = 0.18957802\nIteration 91, loss = 0.18786267\nIteration 92, loss = 0.18771326\nIteration 93, loss = 0.18772981\nIteration 94, loss = 0.18892089\nIteration 95, loss = 0.18849160\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.73605667\nIteration 2, loss = 0.39275019\nIteration 3, loss = 0.29916789\nIteration 4, loss = 0.25426720\nIteration 5, loss = 0.22928895\nIteration 6, loss = 0.21979500\nIteration 7, loss = 0.21474296\nIteration 8, loss = 0.21263429\nIteration 9, loss = 0.20916913\nIteration 10, loss = 0.20765047\nIteration 11, loss = 0.20625644\nIteration 12, loss = 0.20562806\nIteration 13, loss = 0.20597154\nIteration 14, loss = 0.20508235\nIteration 15, loss = 0.20395336\nIteration 16, loss = 0.20323192\nIteration 17, loss = 0.20289968\nIteration 18, loss = 0.20237999\nIteration 19, loss = 0.20046763\nIteration 20, loss = 0.19968508\nIteration 21, loss = 0.20284203\nIteration 22, loss = 0.20236130\nIteration 23, loss = 0.20099309\nIteration 24, loss = 0.19971057\nIteration 25, loss = 0.19633164\nIteration 26, loss = 0.19556659\nIteration 27, loss = 0.19444773\nIteration 28, loss = 0.19435291\nIteration 29, loss = 0.19333931\nIteration 30, loss = 0.19512764\nIteration 31, loss = 0.19540301\nIteration 32, loss = 0.19828660\nIteration 33, loss = 0.19507096\nIteration 34, loss = 0.19264784\nIteration 35, loss = 0.19188671\nIteration 36, loss = 0.19117996\nIteration 37, loss = 0.19066440\nIteration 38, loss = 0.19352686\nIteration 39, loss = 0.19133419\nIteration 40, loss = 0.19159103\nIteration 41, loss = 0.19109741\nIteration 42, loss = 0.19051269\nIteration 43, loss = 0.19078111\nIteration 44, loss = 0.18966758\nIteration 45, loss = 0.19049498\nIteration 46, loss = 0.19009836\nIteration 47, loss = 0.18942500\nIteration 48, loss = 0.19062293\nIteration 49, loss = 0.19078432\nIteration 50, loss = 0.19167190\nIteration 51, loss = 0.18956241\nIteration 52, loss = 0.18952531\nIteration 53, loss = 0.18985467\nIteration 54, loss = 0.18851603\nIteration 55, loss = 0.18879186\nIteration 56, loss = 0.18856051\nIteration 57, loss = 0.18807361\nIteration 58, loss = 0.19024497\nIteration 59, loss = 0.18734333\nIteration 60, loss = 0.19029435\nIteration 61, loss = 0.19189785\nIteration 62, loss = 0.18772241\nIteration 63, loss = 0.18703884\nIteration 64, loss = 0.18674012\nIteration 65, loss = 0.18815102\nIteration 66, loss = 0.18941687\nIteration 67, loss = 0.18797610\nIteration 68, loss = 0.18838067\nIteration 69, loss = 0.18650587\nIteration 70, loss = 0.18728044\nIteration 71, loss = 0.18901463\nIteration 72, loss = 0.18715243\nIteration 73, loss = 0.18707034\nIteration 74, loss = 0.18639441\nIteration 75, loss = 0.18680035\nIteration 76, loss = 0.18667297\nIteration 77, loss = 0.18721876\nIteration 78, loss = 0.18974919\nIteration 79, loss = 0.19036758\nIteration 80, loss = 0.18659802\nIteration 81, loss = 0.18641638\nIteration 82, loss = 0.18786353\nIteration 83, loss = 0.18711305\nIteration 84, loss = 0.18585724\nIteration 85, loss = 0.18709359\nIteration 86, loss = 0.18841045\nIteration 87, loss = 0.18646602\nIteration 88, loss = 0.18607663\nIteration 89, loss = 0.18617626\nIteration 90, loss = 0.18764341\nIteration 91, loss = 0.18627442\nIteration 92, loss = 0.18597327\nIteration 93, loss = 0.18590979\nIteration 94, loss = 0.18605793\nIteration 95, loss = 0.18680900\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.72488905\nIteration 2, loss = 0.39161679\nIteration 3, loss = 0.30181772\nIteration 4, loss = 0.25945198\nIteration 5, loss = 0.23581374\nIteration 6, loss = 0.22813651\nIteration 7, loss = 0.22334467\nIteration 8, loss = 0.21960487\nIteration 9, loss = 0.21620817\nIteration 10, loss = 0.21490765\nIteration 11, loss = 0.21277823\nIteration 12, loss = 0.21352643\nIteration 13, loss = 0.21264110\nIteration 14, loss = 0.20954391\nIteration 15, loss = 0.20958453\nIteration 16, loss = 0.20873895\nIteration 17, loss = 0.20910248\nIteration 18, loss = 0.20753524\nIteration 19, loss = 0.20863252\nIteration 20, loss = 0.20572062\nIteration 21, loss = 0.20828919\nIteration 22, loss = 0.20606995\nIteration 23, loss = 0.20751993\nIteration 24, loss = 0.20890586\nIteration 25, loss = 0.20538874\nIteration 26, loss = 0.20422089\nIteration 27, loss = 0.20417751\nIteration 28, loss = 0.20901616\nIteration 29, loss = 0.20438640\nIteration 30, loss = 0.20420036\nIteration 31, loss = 0.20826304\nIteration 32, loss = 0.20562718\nIteration 33, loss = 0.20541356\nIteration 34, loss = 0.20329992\nIteration 35, loss = 0.20189884\nIteration 36, loss = 0.20273477\nIteration 37, loss = 0.20256853\nIteration 38, loss = 0.20281433\nIteration 39, loss = 0.20156697\nIteration 40, loss = 0.20186648\nIteration 41, loss = 0.19955934\nIteration 42, loss = 0.19904120\nIteration 43, loss = 0.19905417\nIteration 44, loss = 0.19820524\nIteration 45, loss = 0.19696602\nIteration 46, loss = 0.19734713\nIteration 47, loss = 0.19727671\nIteration 48, loss = 0.19853828\nIteration 49, loss = 0.19567860\nIteration 50, loss = 0.19641827\nIteration 51, loss = 0.19476508\nIteration 52, loss = 0.19553389\nIteration 53, loss = 0.19411563\nIteration 54, loss = 0.19365752\nIteration 55, loss = 0.19309386\nIteration 56, loss = 0.19148286\nIteration 57, loss = 0.19263964\nIteration 58, loss = 0.19597492\nIteration 59, loss = 0.19260527\nIteration 60, loss = 0.19364501\nIteration 61, loss = 0.19204915\nIteration 62, loss = 0.19082143\nIteration 63, loss = 0.19269483\nIteration 64, loss = 0.19221676\nIteration 65, loss = 0.19264971\nIteration 66, loss = 0.19247791\nIteration 67, loss = 0.19148867\nIteration 68, loss = 0.19000358\nIteration 69, loss = 0.18998187\nIteration 70, loss = 0.19212121\nIteration 71, loss = 0.19207285\nIteration 72, loss = 0.19247345\nIteration 73, loss = 0.19070193\nIteration 74, loss = 0.19102415\nIteration 75, loss = 0.19033661\nIteration 76, loss = 0.18976570\nIteration 77, loss = 0.19078827\nIteration 78, loss = 0.19330757\nIteration 79, loss = 0.18969873\nIteration 80, loss = 0.18979718\nIteration 81, loss = 0.19073291\nIteration 82, loss = 0.19055665\nIteration 83, loss = 0.19056482\nIteration 84, loss = 0.19046445\nIteration 85, loss = 0.18975040\nIteration 86, loss = 0.19330782\nIteration 87, loss = 0.19028572\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.72321553\nIteration 2, loss = 0.39316667\nIteration 3, loss = 0.30180832\nIteration 4, loss = 0.25786559\nIteration 5, loss = 0.23564116\nIteration 6, loss = 0.22936979\nIteration 7, loss = 0.22376174\nIteration 8, loss = 0.21731134\nIteration 9, loss = 0.21576719\nIteration 10, loss = 0.21338684\nIteration 11, loss = 0.21240742\nIteration 12, loss = 0.21187301\nIteration 13, loss = 0.21242139\nIteration 14, loss = 0.20961995\nIteration 15, loss = 0.20972396\nIteration 16, loss = 0.20892802\nIteration 17, loss = 0.21164143\nIteration 18, loss = 0.20840335\nIteration 19, loss = 0.20949233\nIteration 20, loss = 0.20460305\nIteration 21, loss = 0.20498483\nIteration 22, loss = 0.20314557\nIteration 23, loss = 0.20378769\nIteration 24, loss = 0.20176224\nIteration 25, loss = 0.20072202\nIteration 26, loss = 0.19868895\nIteration 27, loss = 0.19870913\nIteration 28, loss = 0.20249550\nIteration 29, loss = 0.19782969\nIteration 30, loss = 0.19821570\nIteration 31, loss = 0.19845619\nIteration 32, loss = 0.19796316\nIteration 33, loss = 0.19912615\nIteration 34, loss = 0.19748672\nIteration 35, loss = 0.19495786\nIteration 36, loss = 0.19572384\nIteration 37, loss = 0.19604332\nIteration 38, loss = 0.19520238\nIteration 39, loss = 0.19812610\nIteration 40, loss = 0.19496774\nIteration 41, loss = 0.19621784\nIteration 42, loss = 0.19383945\nIteration 43, loss = 0.19470371\nIteration 44, loss = 0.19690118\nIteration 45, loss = 0.19395273\nIteration 46, loss = 0.19392106\nIteration 47, loss = 0.19348692\nIteration 48, loss = 0.19497134\nIteration 49, loss = 0.19383362\nIteration 50, loss = 0.19367947\nIteration 51, loss = 0.19526019\nIteration 52, loss = 0.19573769\nIteration 53, loss = 0.19251371\nIteration 54, loss = 0.19117934\nIteration 55, loss = 0.19082184\nIteration 56, loss = 0.19056290\nIteration 57, loss = 0.19200282\nIteration 58, loss = 0.19291957\nIteration 59, loss = 0.19069590\nIteration 60, loss = 0.19117246\nIteration 61, loss = 0.19256972\nIteration 62, loss = 0.19214021\nIteration 63, loss = 0.19153256\nIteration 64, loss = 0.19204731\nIteration 65, loss = 0.19291594\nIteration 66, loss = 0.19050778\nIteration 67, loss = 0.19085993\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the average score across all folds for Training and Development data\nprint(\"Average score for Training: \", dev_score.mean())\nprint(\"Average score for Development: \", train_score.mean())","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:22:56.770577Z","iopub.execute_input":"2022-12-07T04:22:56.772002Z","iopub.status.idle":"2022-12-07T04:22:56.779127Z","shell.execute_reply.started":"2022-12-07T04:22:56.771946Z","shell.execute_reply":"2022-12-07T04:22:56.777675Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Average score for Training:  0.892\nAverage score for Development:  0.9083455707222523\n","output_type":"stream"}]},{"cell_type":"code","source":"#Check number of 1 and 0 outputs\nx = 0\nfor i in range(df_test.shape[0]):\n    if y_test_pred[i] == 1:\n        x+=1\n\nprint(\"Total: \" + str(len(y_pred)))\nprint(\"1: \" + str(x))\nprint(\"0: \" + str(len(y_pred) - x))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:27:33.661509Z","iopub.execute_input":"2022-12-07T04:27:33.662038Z","iopub.status.idle":"2022-12-07T04:27:33.671563Z","shell.execute_reply.started":"2022-12-07T04:27:33.661999Z","shell.execute_reply":"2022-12-07T04:27:33.670592Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Total: 4000\n1: 1441\n0: 2559\n","output_type":"stream"}]},{"cell_type":"code","source":"#Output results into a file\nfile = open('SiddarthSrinivasan_test_result.txt',\"w\")\ndf_test.reindex()\nfor i in range(df_test.shape[0]):\n    print(df_test.loc[i,'id'],\"\\t\",y_test_pred[i],file = file)\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T04:27:35.550539Z","iopub.execute_input":"2022-12-07T04:27:35.551356Z","iopub.status.idle":"2022-12-07T04:27:35.620385Z","shell.execute_reply.started":"2022-12-07T04:27:35.551296Z","shell.execute_reply":"2022-12-07T04:27:35.619308Z"},"trusted":true},"execution_count":40,"outputs":[]}]}